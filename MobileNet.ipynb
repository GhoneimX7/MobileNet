{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNWetUczgz/iCG1ouc/Ayw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GhoneimX7/MobileNet/blob/master/MobileNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI6Z3x693m5c"
      },
      "outputs": [],
      "source": [
        "from easydict import EasyDict as edict\n",
        "import json\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from pprint import pprint\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loader**\n",
        "As a simple case, the model is tried on TinyImageNet.\n",
        "For larger datasets, you may need to adapt this class to use the Tensorflow Dataset API"
      ],
      "metadata": {
        "id": "NFko_SpV4i_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "\n",
        "    def __init__(self, batch_size, shuffle=False):\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "        self.img_mean = None\n",
        "        self.train_data_len = 0\n",
        "\n",
        "        self.X_val = None\n",
        "        self.y_val = None\n",
        "        self.val_data_len = 0\n",
        "\n",
        "        self.X_test = None\n",
        "        self.y_test = None\n",
        "        self.test_data_len = 0\n",
        "\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def load_data(self):\n",
        "        # Please make sure to change this function to load your train/validation/test data.\n",
        "        train_data = np.array([plt.imread('./data/test_images/0.jpg'), plt.imread('./data/test_images/1.jpg'),\n",
        "                      plt.imread('./data/test_images/2.jpg'), plt.imread('./data/test_images/3.jpg')])\n",
        "        self.X_train = train_data\n",
        "        self.y_train = np.array([284, 264, 682, 2])\n",
        "\n",
        "        val_data = np.array([plt.imread('./data/test_images/0.jpg'), plt.imread('./data/test_images/1.jpg'),\n",
        "                    plt.imread('./data/test_images/2.jpg'), plt.imread('./data/test_images/3.jpg')])\n",
        "\n",
        "        self.X_val = val_data\n",
        "        self.y_val = np.array([284, 264, 682, 2])\n",
        "\n",
        "        self.train_data_len = self.X_train.shape[0]\n",
        "        self.val_data_len = self.X_val.shape[0]\n",
        "        img_height = 224\n",
        "        img_width = 224\n",
        "        num_channels = 3\n",
        "        return img_height, img_width, num_channels, self.train_data_len, self.val_data_len\n",
        "\n",
        "    def generate_batch(self, type='train'):\n",
        "        \"\"\"Generate batch from X_train/X_test and y_train/y_test using a python DataGenerator\"\"\"\n",
        "        if type == 'train':\n",
        "            # Training time!\n",
        "            new_epoch = True\n",
        "            start_idx = 0\n",
        "            mask = None\n",
        "            while True:\n",
        "                if new_epoch:\n",
        "                    start_idx = 0\n",
        "                    if self.shuffle:\n",
        "                        mask = np.random.choice(self.train_data_len, self.train_data_len, replace=False)\n",
        "                    else:\n",
        "                        mask = np.arange(self.train_data_len)\n",
        "                    new_epoch = False\n",
        "\n",
        "                # Batch mask selection\n",
        "                X_batch = self.X_train[mask[start_idx:start_idx + self.batch_size]]\n",
        "                y_batch = self.y_train[mask[start_idx:start_idx + self.batch_size]]\n",
        "                start_idx += self.batch_size\n",
        "\n",
        "                # Reset everything after the end of an epoch\n",
        "                if start_idx >= self.train_data_len:\n",
        "                    new_epoch = True\n",
        "                    mask = None\n",
        "                yield X_batch, y_batch\n",
        "        elif type == 'test':\n",
        "            # Testing time!\n",
        "            start_idx = 0\n",
        "            while True:\n",
        "                # Batch mask selection\n",
        "                X_batch = self.X_test[start_idx:start_idx + self.batch_size]\n",
        "                y_batch = self.y_test[start_idx:start_idx + self.batch_size]\n",
        "                start_idx += self.batch_size\n",
        "\n",
        "                # Reset everything\n",
        "                if start_idx >= self.test_data_len:\n",
        "                    start_idx = 0\n",
        "                yield X_batch, y_batch\n",
        "        elif type == 'val':\n",
        "            # Testing time!\n",
        "            start_idx = 0\n",
        "            while True:\n",
        "                # Batch mask selection\n",
        "                X_batch = self.X_val[start_idx:start_idx + self.batch_size]\n",
        "                y_batch = self.y_val[start_idx:start_idx + self.batch_size]\n",
        "                start_idx += self.batch_size\n",
        "\n",
        "                # Reset everything\n",
        "                if start_idx >= self.val_data_len:\n",
        "                    start_idx = 0\n",
        "                yield X_batch, y_batch\n",
        "        else:\n",
        "            raise ValueError(\"Please select a type from \\'train\\', \\'val\\', or \\'test\\'\")\n"
      ],
      "metadata": {
        "id": "7HzVN5ZN4iqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**layers**"
      ],
      "metadata": {
        "id": "wV_zKsZH5Cy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################################################\n",
        "# Convolution layer Methods\n",
        "def __conv2d_p(name, x, w=None, num_filters=16, kernel_size=(3, 3), padding='SAME', stride=(1, 1),\n",
        "               initializer=tf.compat.v1.keras.initializers.glorot_normal(), l2_strength=0.0, bias=0.0):\n",
        "    \"\"\"\n",
        "    Convolution 2D Wrapper\n",
        "    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.\n",
        "    :param x: (tf.tensor) The input to the layer (N, H, W, C).\n",
        "    :param w: (tf.tensor) pretrained weights (if None, it means no pretrained weights)\n",
        "    :param num_filters: (integer) No. of filters (This is the output depth)\n",
        "    :param kernel_size: (integer tuple) The size of the convolving kernel.\n",
        "    :param padding: (string) The amount of padding required.\n",
        "    :param stride: (integer tuple) The stride required.\n",
        "    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.\n",
        "    :param l2_strength:(weight decay) (float) L2 regularization parameter.\n",
        "    :param bias: (float) Amount of bias. (if not float, it means pretrained bias)\n",
        "    :return out: The output of the layer. (N, H', W', num_filters)\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name):\n",
        "        stride = [1, stride[0], stride[1], 1]\n",
        "        kernel_shape = [kernel_size[0], kernel_size[1], x.shape[-1], num_filters]\n",
        "\n",
        "        with tf.name_scope('layer_weights'):\n",
        "            if w == None:\n",
        "                w = __variable_with_weight_decay(kernel_shape, initializer, l2_strength)\n",
        "            __variable_summaries(w)\n",
        "        with tf.name_scope('layer_biases'):\n",
        "            if isinstance(bias, float):\n",
        "                bias = tf.get_variable('biases', [num_filters], initializer=tf.constant_initializer(bias))\n",
        "            __variable_summaries(bias)\n",
        "        with tf.name_scope('layer_conv2d'):\n",
        "            conv = tf.nn.conv2d(x, w, stride, padding)\n",
        "            out = tf.nn.bias_add(conv, bias)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def conv2d(name, x, w=None, num_filters=16, kernel_size=(3, 3), padding='SAME', stride=(1, 1),\n",
        "           initializer=tf.compat.v1.keras.initializers.glorot_normal(), l2_strength=0.0, bias=0.0,\n",
        "           activation=None, batchnorm_enabled=False, max_pool_enabled=False, dropout_keep_prob=-1,\n",
        "           is_training=True):\n",
        "    \"\"\"\n",
        "    This block is responsible for a convolution 2D layer followed by optional (non-linearity, dropout, max-pooling).\n",
        "    Note that: \"is_training\" should be passed by a correct value based on being in either training or testing.\n",
        "    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.\n",
        "    :param x: (tf.tensor) The input to the layer (N, H, W, C).\n",
        "    :param num_filters: (integer) No. of filters (This is the output depth)\n",
        "    :param kernel_size: (integer tuple) The size of the convolving kernel.\n",
        "    :param padding: (string) The amount of padding required.\n",
        "    :param stride: (integer tuple) The stride required.\n",
        "    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.\n",
        "    :param l2_strength:(weight decay) (float) L2 regularization parameter.\n",
        "    :param bias: (float) Amount of bias.\n",
        "    :param activation: (tf.graph operator) The activation function applied after the convolution operation. If None, linear is applied.\n",
        "    :param batchnorm_enabled: (boolean) for enabling batch normalization.\n",
        "    :param max_pool_enabled:  (boolean) for enabling max-pooling 2x2 to decrease width and height by a factor of 2.\n",
        "    :param dropout_keep_prob: (float) for the probability of keeping neurons. If equals -1, it means no dropout\n",
        "    :param is_training: (boolean) to diff. between training and testing (important for batch normalization and dropout)\n",
        "    :return: The output tensor of the layer (N, H', W', C').\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        conv_o_b = __conv2d_p(scope, x=x, w=w, num_filters=num_filters, kernel_size=kernel_size, stride=stride,\n",
        "                              padding=padding,\n",
        "                              initializer=initializer, l2_strength=l2_strength, bias=bias)\n",
        "\n",
        "        if batchnorm_enabled:\n",
        "            conv_o_bn = tf.layers.batch_normalization(conv_o_b, training=is_training)\n",
        "            if not activation:\n",
        "                conv_a = conv_o_bn\n",
        "            else:\n",
        "                conv_a = activation(conv_o_bn)\n",
        "        else:\n",
        "            if not activation:\n",
        "                conv_a = conv_o_b\n",
        "            else:\n",
        "                conv_a = activation(conv_o_b)\n",
        "\n",
        "        def dropout_with_keep():\n",
        "            return tf.nn.dropout(conv_a, dropout_keep_prob)\n",
        "\n",
        "        def dropout_no_keep():\n",
        "            return tf.nn.dropout(conv_a, 1.0)\n",
        "\n",
        "        if dropout_keep_prob != -1:\n",
        "            conv_o_dr = tf.cond(is_training, dropout_with_keep, dropout_no_keep)\n",
        "        else:\n",
        "            conv_o_dr = conv_a\n",
        "\n",
        "        conv_o = conv_o_dr\n",
        "        if max_pool_enabled:\n",
        "            conv_o = max_pool_2d(conv_o_dr)\n",
        "\n",
        "    return conv_o\n",
        "\n",
        "\n",
        "def __depthwise_conv2d_p(name, x, w=None, kernel_size=(3, 3), padding='SAME', stride=(1, 1),\n",
        "                         initializer=tf.compat.v1.keras.initializers.glorot_normal(), l2_strength=0.0, bias=0.0):\n",
        "    with tf.variable_scope(name):\n",
        "        stride = [1, stride[0], stride[1], 1]\n",
        "        kernel_shape = [kernel_size[0], kernel_size[1], x.shape[-1], 1]\n",
        "\n",
        "        with tf.name_scope('layer_weights'):\n",
        "            if w is None:\n",
        "                w = __variable_with_weight_decay(kernel_shape, initializer, l2_strength)\n",
        "            __variable_summaries(w)\n",
        "        with tf.name_scope('layer_biases'):\n",
        "            if isinstance(bias, float):\n",
        "                bias = tf.get_variable('biases', [x.shape[-1]], initializer=tf.constant_initializer(bias))\n",
        "            __variable_summaries(bias)\n",
        "        with tf.name_scope('layer_conv2d'):\n",
        "            conv = tf.nn.depthwise_conv2d(x, w, stride, padding)\n",
        "            out = tf.nn.bias_add(conv, bias)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def depthwise_conv2d(name, x, w=None, kernel_size=(3, 3), padding='SAME', stride=(1, 1),\n",
        "                     initializer=tf.compat.v1.keras.initializers.glorot_normal(), l2_strength=0.0, bias=0.0, activation=None,\n",
        "                     batchnorm_enabled=False, is_training=True):\n",
        "    \"\"\"Implementation of depthwise 2D convolution wrapper\"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        conv_o_b = __depthwise_conv2d_p(name=scope, x=x, w=w, kernel_size=kernel_size, padding=padding,\n",
        "                                        stride=stride, initializer=initializer, l2_strength=l2_strength, bias=bias)\n",
        "\n",
        "        if batchnorm_enabled:\n",
        "            conv_o_bn = tf.layers.batch_normalization(conv_o_b, training=is_training)\n",
        "            if not activation:\n",
        "                conv_a = conv_o_bn\n",
        "            else:\n",
        "                conv_a = activation(conv_o_bn)\n",
        "        else:\n",
        "            if not activation:\n",
        "                conv_a = conv_o_b\n",
        "            else:\n",
        "                conv_a = activation(conv_o_b)\n",
        "    return conv_a\n",
        "\n",
        "\n",
        "def depthwise_separable_conv2d(name, x, w_depthwise=None, w_pointwise=None, width_multiplier=1.0, num_filters=16,\n",
        "                               kernel_size=(3, 3),\n",
        "                               padding='SAME', stride=(1, 1),\n",
        "                               initializer=tf.compat.v1.keras.initializers.glorot_normal(), l2_strength=0.0, biases=(0.0, 0.0),\n",
        "                               activation=None, batchnorm_enabled=True,\n",
        "                               is_training=True):\n",
        "    \"\"\"Implementation of depthwise separable 2D convolution operator as in MobileNet paper\"\"\"\n",
        "    total_num_filters = int(round(num_filters * width_multiplier))\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        conv_a = depthwise_conv2d('depthwise', x=x, w=w_depthwise, kernel_size=kernel_size, padding=padding,\n",
        "                                  stride=stride,\n",
        "                                  initializer=initializer, l2_strength=l2_strength, bias=biases[0],\n",
        "                                  activation=activation,\n",
        "                                  batchnorm_enabled=batchnorm_enabled, is_training=is_training)\n",
        "\n",
        "        conv_o = conv2d('pointwise', x=conv_a, w=w_pointwise, num_filters=total_num_filters, kernel_size=(1, 1),\n",
        "                        initializer=initializer, l2_strength=l2_strength, bias=biases[1], activation=activation,\n",
        "                        batchnorm_enabled=batchnorm_enabled, is_training=is_training)\n",
        "\n",
        "    return conv_a, conv_o\n",
        "\n",
        "\n",
        "############################################################################################################\n",
        "# Fully Connected layer Methods\n",
        "\n",
        "def __dense_p(name, x, w=None, output_dim=128, initializer=tf.compat.v1.keras.initializers.glorot_normal(), l2_strength=0.0,\n",
        "              bias=0.0):\n",
        "    \"\"\"\n",
        "    Fully connected layer\n",
        "    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.\n",
        "    :param x: (tf.tensor) The input to the layer (N, D).\n",
        "    :param output_dim: (integer) It specifies H, the output second dimension of the fully connected layer [ie:(N, H)]\n",
        "    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.\n",
        "    :param l2_strength:(weight decay) (float) L2 regularization parameter.\n",
        "    :param bias: (float) Amount of bias. (if not float, it means pretrained bias)\n",
        "    :return out: The output of the layer. (N, H)\n",
        "    \"\"\"\n",
        "    n_in = x.get_shape()[-1].value\n",
        "    with tf.variable_scope(name):\n",
        "        if w == None:\n",
        "            w = __variable_with_weight_decay([n_in, output_dim], initializer, l2_strength)\n",
        "        __variable_summaries(w)\n",
        "        if isinstance(bias, float):\n",
        "            bias = tf.get_variable(\"layer_biases\", [output_dim], tf.float32, tf.constant_initializer(bias))\n",
        "        __variable_summaries(bias)\n",
        "        output = tf.nn.bias_add(tf.matmul(x, w), bias)\n",
        "        return output\n",
        "\n",
        "\n",
        "def dense(name, x, w=None, output_dim=128, initializer=tf.compat.v1.keras.initializers.glorot_normal(), l2_strength=0.0,\n",
        "          bias=0.0,\n",
        "          activation=None, batchnorm_enabled=False, dropout_keep_prob=-1,\n",
        "          is_training=True\n",
        "          ):\n",
        "    \"\"\"\n",
        "    This block is responsible for a fully connected followed by optional (non-linearity, dropout, max-pooling).\n",
        "    Note that: \"is_training\" should be passed by a correct value based on being in either training or testing.\n",
        "    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.\n",
        "    :param x: (tf.tensor) The input to the layer (N, D).\n",
        "    :param output_dim: (integer) It specifies H, the output second dimension of the fully connected layer [ie:(N, H)]\n",
        "    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.\n",
        "    :param l2_strength:(weight decay) (float) L2 regularization parameter.\n",
        "    :param bias: (float) Amount of bias.\n",
        "    :param activation: (tf.graph operator) The activation function applied after the convolution operation. If None, linear is applied.\n",
        "    :param batchnorm_enabled: (boolean) for enabling batch normalization.\n",
        "    :param dropout_keep_prob: (float) for the probability of keeping neurons. If equals -1, it means no dropout\n",
        "    :param is_training: (boolean) to diff. between training and testing (important for batch normalization and dropout)\n",
        "    :return out: The output of the layer. (N, H)\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name) as scope:\n",
        "        dense_o_b = __dense_p(name=scope, x=x, w=w, output_dim=output_dim, initializer=initializer,\n",
        "                              l2_strength=l2_strength,\n",
        "                              bias=bias)\n",
        "\n",
        "        if batchnorm_enabled:\n",
        "            dense_o_bn = tf.layers.batch_normalization(dense_o_b, training=is_training)\n",
        "            if not activation:\n",
        "                dense_a = dense_o_bn\n",
        "            else:\n",
        "                dense_a = activation(dense_o_bn)\n",
        "        else:\n",
        "            if not activation:\n",
        "                dense_a = dense_o_b\n",
        "            else:\n",
        "                dense_a = activation(dense_o_b)\n",
        "\n",
        "        def dropout_with_keep():\n",
        "            return tf.nn.dropout(dense_a, dropout_keep_prob)\n",
        "\n",
        "        def dropout_no_keep():\n",
        "            return tf.nn.dropout(dense_a, 1.0)\n",
        "\n",
        "        if dropout_keep_prob != -1:\n",
        "            dense_o_dr = tf.cond(is_training, dropout_with_keep, dropout_no_keep)\n",
        "        else:\n",
        "            dense_o_dr = dense_a\n",
        "\n",
        "        dense_o = dense_o_dr\n",
        "    return dense_o\n",
        "\n",
        "\n",
        "def dropout(x, dropout_keep_prob, is_training):\n",
        "    \"\"\"Dropout special layer\"\"\"\n",
        "\n",
        "    def dropout_with_keep():\n",
        "        return tf.nn.dropout(x, dropout_keep_prob)\n",
        "\n",
        "    def dropout_no_keep():\n",
        "        return tf.nn.dropout(x, 1.0)\n",
        "\n",
        "    if dropout_keep_prob != -1:\n",
        "        output = tf.cond(is_training, dropout_with_keep, dropout_no_keep)\n",
        "    else:\n",
        "        output = x\n",
        "    return output\n",
        "\n",
        "\n",
        "def flatten(x):\n",
        "    \"\"\"\n",
        "    Flatten a (N,H,W,C) input into (N,D) output. Used for fully connected layers after conolution layers\n",
        "    :param x: (tf.tensor) representing input\n",
        "    :return: flattened output\n",
        "    \"\"\"\n",
        "    all_dims_exc_first = np.prod([v.value for v in x.get_shape()[1:]])\n",
        "    o = tf.reshape(x, [-1, all_dims_exc_first])\n",
        "    return o\n",
        "\n",
        "\n",
        "############################################################################################################\n",
        "# Pooling Methods\n",
        "\n",
        "def max_pool_2d(x, size=(2, 2), stride=(2, 2), name='pooling'):\n",
        "    \"\"\"\n",
        "    Max pooling 2D Wrapper\n",
        "    :param x: (tf.tensor) The input to the layer (N,H,W,C).\n",
        "    :param size: (tuple) This specifies the size of the filter as well as the stride.\n",
        "    :param stride: (tuple) specifies the stride of pooling.\n",
        "    :param name: (string) Scope name.\n",
        "    :return: The output is the same input but halfed in both width and height (N,H/2,W/2,C).\n",
        "    \"\"\"\n",
        "    size_x, size_y = size\n",
        "    stride_x, stride_y = stride\n",
        "    return tf.nn.max_pool(x, ksize=[1, size_x, size_y, 1], strides=[1, stride_x, stride_y, 1], padding='VALID',\n",
        "                          name=name)\n",
        "\n",
        "\n",
        "def avg_pool_2d(x, size=(2, 2), stride=(2, 2), name='avg_pooling'):\n",
        "    \"\"\"\n",
        "        Average pooling 2D Wrapper\n",
        "        :param x: (tf.tensor) The input to the layer (N,H,W,C).\n",
        "        :param size: (tuple) This specifies the size of the filter as well as the stride.\n",
        "        :param name: (string) Scope name.\n",
        "        :return: The output is the same input but halfed in both width and height (N,H/2,W/2,C).\n",
        "    \"\"\"\n",
        "    size_x, size_y = size\n",
        "    stride_x, stride_y = stride\n",
        "    return tf.nn.avg_pool(x, ksize=[1, size_x, size_y, 1], strides=[1, stride_x, stride_y, 1], padding='VALID',\n",
        "                          name=name)\n",
        "\n",
        "\n",
        "############################################################################################################\n",
        "# Utilities for layers\n",
        "\n",
        "def __variable_with_weight_decay(kernel_shape, initializer, wd):\n",
        "    \"\"\"\n",
        "    Create a variable with L2 Regularization (Weight Decay)\n",
        "    :param kernel_shape: the size of the convolving weight kernel.\n",
        "    :param initializer: The initialization scheme, He et al. normal or Xavier normal are recommended.\n",
        "    :param wd:(weight decay) L2 regularization parameter.\n",
        "    :return: The weights of the kernel initialized. The L2 loss is added to the loss collection.\n",
        "    \"\"\"\n",
        "    w = tf.get_variable('weights', kernel_shape, tf.float32, initializer=initializer)\n",
        "\n",
        "    collection_name = tf.GraphKeys.REGULARIZATION_LOSSES\n",
        "    if wd and (not tf.get_variable_scope().reuse):\n",
        "        weight_decay = tf.multiply(tf.nn.l2_loss(w), wd, name='w_loss')\n",
        "        tf.add_to_collection(collection_name, weight_decay)\n",
        "    return w\n",
        "\n",
        "\n",
        "# Summaries for variables\n",
        "def __variable_summaries(var):\n",
        "    \"\"\"\n",
        "    Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n",
        "    :param var: variable to be summarized\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    with tf.name_scope('summaries'):\n",
        "        mean = tf.reduce_mean(var)\n",
        "        tf.summary.scalar('mean', mean)\n",
        "        with tf.name_scope('stddev'):\n",
        "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "        tf.summary.scalar('stddev', stddev)\n",
        "        tf.summary.scalar('max', tf.reduce_max(var))\n",
        "        tf.summary.scalar('min', tf.reduce_min(var))\n",
        "        tf.summary.histogram('histogram', var)\n"
      ],
      "metadata": {
        "id": "uaXUX8rm5Ind"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utils**"
      ],
      "metadata": {
        "id": "hF-elmWv5Zdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "    \"\"\"\n",
        "    Parse the arguments of the program\n",
        "    :return: (config_args)\n",
        "    :rtype: tuple\n",
        "    \"\"\"\n",
        "    # Create a parser\n",
        "    parser = argparse.ArgumentParser(description=\"MobileNet TensorFlow Implementation\")\n",
        "    parser.add_argument('--version', action='version', version='%(prog)s 1.0.0')\n",
        "    parser.add_argument('--config', default=None, type=str, help='Configuration file')\n",
        "\n",
        "    # Parse the arguments\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Parse the configurations from the config json file provided\n",
        "    try:\n",
        "        if args.config is not None:\n",
        "            with open(args.config, 'r') as config_file:\n",
        "                config_args_dict = json.load(config_file)\n",
        "        else:\n",
        "            print(\"Add a config file using \\'--config file_name.json\\'\", file=sys.stderr)\n",
        "            exit(1)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"ERROR: Config file not found: {}\".format(args.config), file=sys.stderr)\n",
        "        exit(1)\n",
        "    except json.decoder.JSONDecodeError:\n",
        "        print(\"ERROR: Config file is not a proper JSON file!\", file=sys.stderr)\n",
        "        exit(1)\n",
        "\n",
        "    config_args = edict(config_args_dict)\n",
        "\n",
        "    pprint(config_args)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return config_args\n",
        "\n",
        "\n",
        "def create_experiment_dirs(exp_dir):\n",
        "    \"\"\"\n",
        "    Create Directories of a regular tensorflow experiment directory\n",
        "    :param exp_dir:\n",
        "    :return summary_dir, checkpoint_dir:\n",
        "    \"\"\"\n",
        "    experiment_dir = os.path.realpath(os.path.join(os.path.dirname(__file__))) + \"/experiments/\" + exp_dir + \"/\"\n",
        "    summary_dir = experiment_dir + 'summaries/'\n",
        "    checkpoint_dir = experiment_dir + 'checkpoints/'\n",
        "    dirs = [summary_dir, checkpoint_dir]\n",
        "    try:\n",
        "        for dir_ in dirs:\n",
        "            if not os.path.exists(dir_):\n",
        "                os.makedirs(dir_)\n",
        "        print(\"Experiment directories created!\")\n",
        "        # return experiment_dir, summary_dir, checkpoint_dir, output_dir, test_dir\n",
        "        return experiment_dir, summary_dir, checkpoint_dir\n",
        "    except Exception as err:\n",
        "        print(\"Creating directories error: {0}\".format(err))\n",
        "        exit(-1)\n",
        "\n",
        "\n",
        "def load_obj(name):\n",
        "    with open(name, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "\n",
        "def save_obj(obj, name):\n",
        "    with open(name, 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def calculate_flops():\n",
        "    # Print to stdout an analysis of the number of floating point operations in the\n",
        "    # model broken down by individual operations.\n",
        "    tf.profiler.profile(\n",
        "        tf.get_default_graph(),\n",
        "        options=tf.profiler.ProfileOptionBuilder.float_operation(), cmd='scope')\n"
      ],
      "metadata": {
        "id": "qCGPnV5K5Y6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**"
      ],
      "metadata": {
        "id": "CeFvu5CS5neK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MobileNet:\n",
        "    \"\"\"\n",
        "    MobileNet Class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 args):\n",
        "\n",
        "        # init parameters and input\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        self.logits = None\n",
        "        self.is_training = None\n",
        "        self.loss = None\n",
        "        self.regularization_loss = None\n",
        "        self.cross_entropy_loss = None\n",
        "        self.train_op = None\n",
        "        self.accuracy = None\n",
        "        self.y_out_argmax = None\n",
        "        self.summaries_merged = None\n",
        "        self.args = args\n",
        "        self.mean_img = None\n",
        "        self.nodes = dict()\n",
        "\n",
        "        self.pretrained_path = os.path.realpath(self.args.pretrained_path)\n",
        "\n",
        "        self.__build()\n",
        "\n",
        "    def __init_input(self):\n",
        "        with tf.variable_scope('input'):\n",
        "            # Input images\n",
        "            self.X = tf.placeholder(tf.float32,\n",
        "                                    [self.args.batch_size, self.args.img_height, self.args.img_width,\n",
        "                                     self.args.num_channels])\n",
        "            # Classification supervision, it's an argmax. Feel free to change it to one-hot,\n",
        "            # but don't forget to change the loss from sparse as well\n",
        "            self.y = tf.placeholder(tf.int32, [self.args.batch_size])\n",
        "            # is_training is for batch normalization and dropout, if they exist\n",
        "            self.is_training = tf.placeholder(tf.bool)\n",
        "\n",
        "    def __init_mean(self):\n",
        "        # Preparing the mean image.\n",
        "        img_mean = np.ones((1, 224, 224, 3))\n",
        "        img_mean[:, :, :, 0] *= 103.939\n",
        "        img_mean[:, :, :, 1] *= 116.779\n",
        "        img_mean[:, :, :, 2] *= 123.68\n",
        "        self.mean_img = tf.constant(img_mean, dtype=tf.float32)\n",
        "\n",
        "    def __build(self):\n",
        "        self.__init_global_epoch()\n",
        "        self.__init_global_step()\n",
        "        self.__init_mean()\n",
        "        self.__init_input()\n",
        "        self.__init_network()\n",
        "        self.__init_output()\n",
        "\n",
        "    def __init_network(self):\n",
        "        with tf.variable_scope('mobilenet_encoder'):\n",
        "            # Preprocessing as done in the paper\n",
        "            with tf.name_scope('pre_processing'):\n",
        "                preprocessed_input = (self.X - self.mean_img) / 255.0\n",
        "\n",
        "            # Model is here!\n",
        "            conv1_1 = conv2d('conv_1', preprocessed_input, num_filters=int(round(32 * self.args.width_multiplier)),\n",
        "                             kernel_size=(3, 3),\n",
        "                             padding='SAME', stride=(2, 2), activation=tf.nn.relu6,\n",
        "                             batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                             is_training=self.is_training, l2_strength=self.args.l2_strength, bias=self.args.bias)\n",
        "            self.__add_to_nodes([conv1_1])\n",
        "            ############################################################################################\n",
        "            conv2_1_dw, conv2_1_pw = depthwise_separable_conv2d('conv_ds_2', conv1_1,\n",
        "                                                                width_multiplier=self.args.width_multiplier,\n",
        "                                                                num_filters=64, kernel_size=(3, 3), padding='SAME',\n",
        "                                                                stride=(1, 1),\n",
        "                                                                batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                                                                activation=tf.nn.relu6,\n",
        "                                                                is_training=self.is_training,\n",
        "                                                                l2_strength=self.args.l2_strength,\n",
        "                                                                biases=(self.args.bias, self.args.bias))\n",
        "            self.__add_to_nodes([conv2_1_dw, conv2_1_pw])\n",
        "\n",
        "            conv2_2_dw, conv2_2_pw = depthwise_separable_conv2d('conv_ds_3', conv2_1_pw,\n",
        "                                                                width_multiplier=self.args.width_multiplier,\n",
        "                                                                num_filters=128, kernel_size=(3, 3), padding='SAME',\n",
        "                                                                stride=(2, 2),\n",
        "                                                                batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                                                                activation=tf.nn.relu6,\n",
        "                                                                is_training=self.is_training,\n",
        "                                                                l2_strength=self.args.l2_strength,\n",
        "                                                                biases=(self.args.bias, self.args.bias))\n",
        "            self.__add_to_nodes([conv2_2_dw, conv2_2_pw])\n",
        "            ############################################################################################\n",
        "            conv3_1_dw, conv3_1_pw = depthwise_separable_conv2d('conv_ds_4', conv2_2_pw,\n",
        "                                                                width_multiplier=self.args.width_multiplier,\n",
        "                                                                num_filters=128, kernel_size=(3, 3), padding='SAME',\n",
        "                                                                stride=(1, 1),\n",
        "                                                                batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                                                                activation=tf.nn.relu6,\n",
        "                                                                is_training=self.is_training,\n",
        "                                                                l2_strength=self.args.l2_strength,\n",
        "                                                                biases=(self.args.bias, self.args.bias))\n",
        "            self.__add_to_nodes([conv3_1_dw, conv3_1_pw])\n",
        "\n",
        "            conv3_2_dw, conv3_2_pw = depthwise_separable_conv2d('conv_ds_5', conv3_1_pw,\n",
        "                                                                width_multiplier=self.args.width_multiplier,\n",
        "                                                                num_filters=256, kernel_size=(3, 3), padding='SAME',\n",
        "                                                                stride=(2, 2),\n",
        "                                                                batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                                                                activation=tf.nn.relu6,\n",
        "                                                                is_training=self.is_training,\n",
        "                                                                l2_strength=self.args.l2_strength,\n",
        "                                                                biases=(self.args.bias, self.args.bias))\n",
        "            self.__add_to_nodes([conv3_2_dw, conv3_2_pw])\n",
        "            ############################################################################################\n",
        "            conv4_1_dw, conv4_1_pw = depthwise_separable_conv2d('conv_ds_6', conv3_2_pw,\n",
        "                                                                width_multiplier=self.args.width_multiplier,\n",
        "                                                                num_filters=256, kernel_size=(3, 3), padding='SAME',\n",
        "                                                                stride=(1, 1),\n",
        "                                                                batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                                                                activation=tf.nn.relu6,\n",
        "                                                                is_training=self.is_training,\n",
        "                                                                l2_strength=self.args.l2_strength,\n",
        "                                                                biases=(self.args.bias, self.args.bias))\n",
        "            self.__add_to_nodes([conv4_1_dw, conv4_1_pw])\n",
        "\n",
        "            conv4_2_dw, conv4_2_pw = depthwise_separable_conv2d('conv_ds_7', conv4_1_pw,\n",
        "                                                                width_multiplier=self.args.width_multiplier,\n",
        "                                                                num_filters=512, kernel_size=(3, 3), padding='SAME',\n",
        "                                                                stride=(2, 2),\n",
        "                                                                batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                                                                activation=tf.nn.relu6,\n",
        "                                                                is_training=self.is_training,\n",
        "                                                                l2_strength=self.args.l2_strength,\n",
        "                                                                biases=(self.args.bias, self.args.bias))\n",
        "            self.__add_to_nodes([conv4_2_dw, conv4_2_pw])\n",
        "            ############################################################################################\n",
        "            conv5_1_dw, conv5_1_pw = depthwise_separable_conv2d('conv_ds_8', conv4_2_pw,\n",
        "                                                                width_multiplier=self.args.width_multiplier,\n",
        "                                                                num_filters=512, kernel_size=(3, 3), padding='SAME',\n",
        "                                                                stride=(1, 1),\n",
        "                                                                batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                                                                activation=tf.nn.relu6,\n",
        "                                                                is_training=self.is_training,\n",
        "                                                                l2_strength=self.args.l2_strength,\n",
        "                                                                biases=(self.args.bias, self.args.bias))\n",
        "            self.__add_to_nodes([conv5_1_dw, conv5_1_pw])\n",
        "\n",
        "            conv5_2_dw, conv5_2_pw = depthwise_separable_conv2d('conv_ds_9', conv5_1_pw,\n",
        "                                                                width_multiplier=self.args.width_multiplier,\n",
        "                                                                num_filters=512, kernel_size=(3, 3), padding='SAME',\n",
        "                                                                stride=(1, 1),\n",
        "                                                                batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                                                                activation=tf.nn.relu6,\n",
        "                                                                is_training=self.is_training,\n",
        "                                                                l2_strength=self.args.l2_strength,\n",
        "                                                                biases=(self.args.bias, self.args.bias))\n",
        "            self.__add_to_nodes([conv5_2_dw, conv5_2_pw])\n",
        "\n",
        "            conv5_3_dw, conv5_3_pw = depthwise_separable_conv2d('conv_ds_10', conv5_2_pw,\n",
        "                                                                width_multiplier=self.args.width_multiplier,\n",
        "                                                                num_filters=512, kernel_size=(3, 3), padding='SAME',\n",
        "                                                                stride=(1, 1),\n",
        "                                                                batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                                                                activation=tf.nn.relu6,\n",
        "                                                                is_training=self.is_training,\n",
        "                                                                l2_strength=self.args.l2_strength,\n",
        "                                                                biases=(self.args.bias, self.args.bias))\n",
        "            self.__add_to_nodes([conv5_3_dw, conv5_3_pw])\n",
        "\n",
        "            conv5_4_dw, conv5_4_pw = depthwise_separable_conv2d('conv_ds_11', conv5_3_pw,\n",
        "                                                                width_multiplier=self.args.width_multiplier,\n",
        "                                                                num_filters=512, kernel_size=(3, 3), padding='SAME',\n",
        "                                                                stride=(1, 1),\n",
        "                                                                batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                                                                activation=tf.nn.relu6,\n",
        "                                                                is_training=self.is_training,\n",
        "                                                                l2_strength=self.args.l2_strength,\n",
        "                                                                biases=(self.args.bias, self.args.bias))\n",
        "            self.__add_to_nodes([conv5_4_dw, conv5_4_pw])\n",
        "\n",
        "            conv5_5_dw, conv5_5_pw = depthwise_separable_conv2d('conv_ds_12', conv5_4_pw,\n",
        "                                                                width_multiplier=self.args.width_multiplier,\n",
        "                                                                num_filters=512, kernel_size=(3, 3), padding='SAME',\n",
        "                                                                stride=(1, 1),\n",
        "                                                                batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                                                                activation=tf.nn.relu6,\n",
        "                                                                is_training=self.is_training,\n",
        "                                                                l2_strength=self.args.l2_strength,\n",
        "                                                                biases=(self.args.bias, self.args.bias))\n",
        "            self.__add_to_nodes([conv5_5_dw, conv5_5_pw])\n",
        "\n",
        "            conv5_6_dw, conv5_6_pw = depthwise_separable_conv2d('conv_ds_13', conv5_5_pw,\n",
        "                                                                width_multiplier=self.args.width_multiplier,\n",
        "                                                                num_filters=1024, kernel_size=(3, 3), padding='SAME',\n",
        "                                                                stride=(2, 2),\n",
        "                                                                batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                                                                activation=tf.nn.relu6,\n",
        "                                                                is_training=self.is_training,\n",
        "                                                                l2_strength=self.args.l2_strength,\n",
        "                                                                biases=(self.args.bias, self.args.bias))\n",
        "            self.__add_to_nodes([conv5_6_dw, conv5_6_pw])\n",
        "            ############################################################################################\n",
        "            conv6_1_dw, conv6_1_pw = depthwise_separable_conv2d('conv_ds_14', conv5_6_pw,\n",
        "                                                                width_multiplier=self.args.width_multiplier,\n",
        "                                                                num_filters=1024, kernel_size=(3, 3), padding='SAME',\n",
        "                                                                stride=(1, 1),\n",
        "                                                                batchnorm_enabled=self.args.batchnorm_enabled,\n",
        "                                                                activation=tf.nn.relu6,\n",
        "                                                                is_training=self.is_training,\n",
        "                                                                l2_strength=self.args.l2_strength,\n",
        "                                                                biases=(self.args.bias, self.args.bias))\n",
        "            self.__add_to_nodes([conv6_1_dw, conv6_1_pw])\n",
        "            ############################################################################################\n",
        "            avg_pool = avg_pool_2d(conv6_1_pw, size=(7, 7), stride=(1, 1))\n",
        "            dropped = dropout(avg_pool, self.args.dropout_keep_prob, self.is_training)\n",
        "            self.logits = flatten(conv2d('fc', dropped, kernel_size=(1, 1), num_filters=self.args.num_classes,\n",
        "                                         l2_strength=self.args.l2_strength,\n",
        "                                         bias=self.args.bias))\n",
        "            self.__add_to_nodes([avg_pool, dropped, self.logits])\n",
        "\n",
        "\n",
        "    def __init_output(self):\n",
        "        with tf.variable_scope('output'):\n",
        "            self.regularization_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
        "            self.cross_entropy_loss = tf.reduce_mean(\n",
        "                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y, name='loss'))\n",
        "            self.loss = self.regularization_loss + self.cross_entropy_loss\n",
        "\n",
        "            # Important for Batch Normalization\n",
        "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "            with tf.control_dependencies(update_ops):\n",
        "                self.train_op = tf.train.AdamOptimizer(learning_rate=self.args.learning_rate).minimize(self.loss)\n",
        "            self.y_out_argmax = tf.argmax(tf.nn.softmax(self.logits), axis=-1, output_type=tf.int32)\n",
        "\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.y, self.y_out_argmax), tf.float32))\n",
        "\n",
        "        # Summaries needed for TensorBoard\n",
        "        with tf.name_scope('train-summary-per-iteration'):\n",
        "            tf.summary.scalar('loss', self.loss)\n",
        "            tf.summary.scalar('acc', self.accuracy)\n",
        "            self.summaries_merged = tf.summary.merge_all()\n",
        "\n",
        "    def __restore(self, file_name, sess):\n",
        "        try:\n",
        "            print(\"Loading ImageNet pretrained weights...\")\n",
        "            variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='mobilenet_encoder')\n",
        "            dict = load_obj(file_name)\n",
        "            run_list = []\n",
        "            for variable in variables:\n",
        "                for key, value in dict.items():\n",
        "                    if key in variable.name:\n",
        "                        run_list.append(tf.assign(variable, value))\n",
        "            sess.run(run_list)\n",
        "            print(\"ImageNet Pretrained Weights Loaded Initially\\n\\n\")\n",
        "        except:\n",
        "            print(\"No pretrained ImageNet weights exist. Skipping...\\n\\n\")\n",
        "\n",
        "    def load_pretrained_weights(self, sess):\n",
        "        self.__restore(self.pretrained_path, sess)\n",
        "\n",
        "    def __add_to_nodes(self, nodes):\n",
        "        for node in nodes:\n",
        "            self.nodes[node.name] = node\n",
        "\n",
        "    def __init_global_epoch(self):\n",
        "        \"\"\"\n",
        "        Create a global epoch tensor to totally save the process of the training\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        with tf.variable_scope('global_epoch'):\n",
        "            self.global_epoch_tensor = tf.Variable(-1, trainable=False, name='global_epoch')\n",
        "            self.global_epoch_input = tf.placeholder('int32', None, name='global_epoch_input')\n",
        "            self.global_epoch_assign_op = self.global_epoch_tensor.assign(self.global_epoch_input)\n",
        "\n",
        "    def __init_global_step(self):\n",
        "        \"\"\"\n",
        "        Create a global step variable to be a reference to the number of iterations\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        with tf.variable_scope('global_step'):\n",
        "            self.global_step_tensor = tf.Variable(0, trainable=False, name='global_step')\n",
        "            self.global_step_input = tf.placeholder('int32', None, name='global_step_input')\n",
        "            self.global_step_assign_op = self.global_step_tensor.assign(self.global_step_input)\n"
      ],
      "metadata": {
        "id": "QgfjlNnb5tQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarizer**"
      ],
      "metadata": {
        "id": "GdIFvsjN55b4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Summarizer:\n",
        "    \"\"\"The class responsible for Tensorboard summaries such as loss, and classification accuracy\"\"\"\n",
        "\n",
        "    def __init__(self, sess, summary_dir):\n",
        "        # Summaries\n",
        "        self.sess = sess\n",
        "        self.scalar_summary_tags = ['loss', 'acc', 'test-loss', 'test-acc']\n",
        "        self.summary_tags = []\n",
        "        self.summary_placeholders = {}\n",
        "        self.summary_ops = {}\n",
        "        self.summary_writer = tf.summary.FileWriter(summary_dir, self.sess.graph)\n",
        "        self.__init_summaries()\n",
        "\n",
        "    ############################################################################################################\n",
        "    # Summaries methods\n",
        "    def __init_summaries(self):\n",
        "        \"\"\"\n",
        "        Create the summary part of the graph\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        with tf.variable_scope('train-summary-per-epoch'):\n",
        "            for tag in self.scalar_summary_tags:\n",
        "                self.summary_tags += tag\n",
        "                self.summary_placeholders[tag] = tf.placeholder('float32', None, name=tag)\n",
        "                self.summary_ops[tag] = tf.summary.scalar(tag, self.summary_placeholders[tag])\n",
        "\n",
        "    def add_summary(self, step, summaries_dict=None, summaries_merged=None):\n",
        "        \"\"\"\n",
        "        Add the summaries to tensorboard\n",
        "        :param step:\n",
        "        :param summaries_dict:\n",
        "        :param summaries_merged:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if summaries_dict is not None:\n",
        "            summary_list = self.sess.run([self.summary_ops[tag] for tag in summaries_dict.keys()],\n",
        "                                         {self.summary_placeholders[tag]: value for tag, value in\n",
        "                                          summaries_dict.items()})\n",
        "            for summary in summary_list:\n",
        "                self.summary_writer.add_summary(summary, step)\n",
        "        if summaries_merged is not None:\n",
        "            self.summary_writer.add_summary(summaries_merged, step)\n"
      ],
      "metadata": {
        "id": "y4eD9zvk6Agc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train**"
      ],
      "metadata": {
        "id": "f8VpmsRp6CrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Train:\n",
        "    \"\"\"Trainer class for the CNN.\n",
        "    It's also responsible for loading/saving the model checkpoints from/to experiments/experiment_name/checkpoint_dir\"\"\"\n",
        "\n",
        "    def __init__(self, sess, model, data, summarizer):\n",
        "        self.sess = sess\n",
        "        self.model = model\n",
        "        self.args = self.model.args\n",
        "        self.saver = tf.train.Saver(max_to_keep=self.args.max_to_keep,\n",
        "                                    keep_checkpoint_every_n_hours=10,\n",
        "                                    save_relative_paths=True)\n",
        "        # Summarizer references\n",
        "        self.data = data\n",
        "        self.summarizer = summarizer\n",
        "\n",
        "        # Initializing the model\n",
        "        self.init = None\n",
        "        self.__init_model()\n",
        "\n",
        "        # Loading the model checkpoint if exists\n",
        "        self.__load_model()\n",
        "\n",
        "    ############################################################################################################\n",
        "    # Model related methods\n",
        "    def __init_model(self):\n",
        "        print(\"Initializing the model...\")\n",
        "        self.init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
        "        self.sess.run(self.init)\n",
        "        print(\"Model initialized successfully\\n\\n\")\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"\n",
        "        Save Model Checkpoint\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        print(\"Saving a checkpoint\")\n",
        "        self.saver.save(self.sess, self.args.checkpoint_dir, self.model.global_step_tensor)\n",
        "        print(\"Checkpoint Saved\\n\\n\")\n",
        "\n",
        "    def __load_model(self):\n",
        "        self.model.load_pretrained_weights(self.sess)\n",
        "\n",
        "        latest_checkpoint = tf.train.latest_checkpoint(self.args.checkpoint_dir)\n",
        "        if latest_checkpoint:\n",
        "            print(\"Loading model checkpoint {} ...\\n\".format(latest_checkpoint))\n",
        "            self.saver.restore(self.sess, latest_checkpoint)\n",
        "            print(\"Checkpoint loaded\\n\\n\")\n",
        "        else:\n",
        "            print(\"No checkpoints available!\\n\\n\")\n",
        "\n",
        "    ############################################################################################################\n",
        "    # Train and Test methods\n",
        "    def train(self):\n",
        "        for cur_epoch in range(self.model.global_epoch_tensor.eval(self.sess) + 1, self.args.num_epochs + 1, 1):\n",
        "\n",
        "            # Initialize tqdm\n",
        "            num_iterations = self.args.train_data_size // self.args.batch_size\n",
        "            tqdm_batch = tqdm(self.data.generate_batch(type='train'), total=num_iterations,\n",
        "                              desc=\"Epoch-\" + str(cur_epoch) + \"-\")\n",
        "\n",
        "            # Initialize the current iterations\n",
        "            cur_iteration = 0\n",
        "\n",
        "            # Initialize classification accuracy and loss lists\n",
        "            loss_list = []\n",
        "            acc_list = []\n",
        "\n",
        "            # Loop by the number of iterations\n",
        "            for X_batch, y_batch in tqdm_batch:\n",
        "                # Get the current iteration for summarizing it\n",
        "                cur_step = self.model.global_step_tensor.eval(self.sess)\n",
        "\n",
        "                # Feed this variables to the network\n",
        "                feed_dict = {self.model.X: X_batch,\n",
        "                             self.model.y: y_batch,\n",
        "                             self.model.is_training: True\n",
        "                             }\n",
        "                # Run the feed_forward\n",
        "                _, loss, acc, summaries_merged = self.sess.run(\n",
        "                    [self.model.train_op, self.model.loss, self.model.accuracy, self.model.summaries_merged],\n",
        "                    feed_dict=feed_dict)\n",
        "                # Append loss and accuracy\n",
        "                loss_list += [loss]\n",
        "                acc_list += [acc]\n",
        "\n",
        "                # Update the Global step\n",
        "                self.model.global_step_assign_op.eval(session=self.sess,\n",
        "                                                      feed_dict={self.model.global_step_input: cur_step + 1})\n",
        "\n",
        "                self.summarizer.add_summary(cur_step, summaries_merged=summaries_merged)\n",
        "\n",
        "                if cur_iteration >= num_iterations - 1:\n",
        "                    avg_loss = np.mean(loss_list)\n",
        "                    avg_acc = np.mean(acc_list)\n",
        "                    # summarize\n",
        "                    summaries_dict = dict()\n",
        "                    summaries_dict['loss'] = avg_loss\n",
        "                    summaries_dict['acc'] = avg_acc\n",
        "\n",
        "                    # summarize\n",
        "                    self.summarizer.add_summary(cur_step, summaries_dict=summaries_dict)\n",
        "\n",
        "                    # Update the Current Epoch tensor\n",
        "                    self.model.global_epoch_assign_op.eval(session=self.sess,\n",
        "                                                           feed_dict={self.model.global_epoch_input: cur_epoch + 1})\n",
        "\n",
        "                    # Print in console\n",
        "                    tqdm_batch.close()\n",
        "                    print(\"Epoch-\" + str(cur_epoch) + \" | \" + \"loss: \" + str(avg_loss) + \" -\" + \" acc: \" + str(\n",
        "                        avg_acc)[\n",
        "                                                                                                           :7])\n",
        "                    # Break the loop to finalize this epoch\n",
        "                    break\n",
        "\n",
        "                # Update the current iteration\n",
        "                cur_iteration += 1\n",
        "\n",
        "            # Save the current checkpoint\n",
        "            if cur_epoch % self.args.save_model_every == 0:\n",
        "                self.save_model()\n",
        "\n",
        "            # Test the model on validation or test data\n",
        "            if cur_epoch % self.args.test_every == 0:\n",
        "                self.test('val')\n",
        "                pass\n",
        "\n",
        "    def test(self, test_type='val'):\n",
        "        num_iterations = self.args.test_data_size // self.args.batch_size\n",
        "        tqdm_batch = tqdm(self.data.generate_batch(type=test_type), total=num_iterations,\n",
        "                          desc='Testing')\n",
        "        # Initialize classification accuracy and loss lists\n",
        "        loss_list = []\n",
        "        acc_list = []\n",
        "        cur_iteration = 0\n",
        "\n",
        "        for X_batch, y_batch in tqdm_batch:\n",
        "            # Feed this variables to the network\n",
        "            feed_dict = {self.model.X: X_batch,\n",
        "                         self.model.y: y_batch,\n",
        "                         self.model.is_training: False\n",
        "                         }\n",
        "            # Run the feed_forward\n",
        "            # Nodes are important for debugging as they dump all the graph!\n",
        "            loss, acc, argmax, nodes = self.sess.run(\n",
        "                [self.model.loss, self.model.accuracy, self.model.y_out_argmax, self.model.nodes],\n",
        "                feed_dict=feed_dict)\n",
        "\n",
        "            # Append loss and accuracy\n",
        "            loss_list += [loss]\n",
        "            acc_list += [acc]\n",
        "\n",
        "            if cur_iteration >= num_iterations - 1:\n",
        "                avg_loss = np.mean(loss_list)\n",
        "                avg_acc = np.mean(acc_list)\n",
        "                print('Test results | test_loss: ' + str(avg_loss) + ' - test_acc: ' + str(avg_acc)[:7])\n",
        "                break\n",
        "\n",
        "            cur_iteration += 1\n"
      ],
      "metadata": {
        "id": "qcb-a4h16Ipf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test.json**"
      ],
      "metadata": {
        "id": "WJ2Qr0zo6Mw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"experiment_dir\": \"test_experiment\",\n",
        "  \"num_epochs\": 100,\n",
        "  \"num_classes\": 1001,\n",
        "  \"batch_size\": 1,\n",
        "  \"width_multiplier\": 1.0,\n",
        "  \"shuffle\": True,\n",
        "  \"l2_strength\": 4e-5,\n",
        "  \"bias\": 0.0,\n",
        "  \"learning_rate\": 1e-3,\n",
        "  \"batchnorm_enabled\": True,\n",
        "  \"dropout_keep_prob\": 0.999,\n",
        "  \"pretrained_path\": \"pretrained_weights/mobilenet_v1.pkl\",\n",
        "  \"max_to_keep\": 4,\n",
        "  \"save_model_every\": 5,\n",
        "  \"test_every\": 5,\n",
        "  \"to_train\": False,\n",
        "  \"to_test\": True\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgN8-6Fl6Q5B",
        "outputId": "4ca665f0-976a-434a-ada4-114fe9bc2db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 1,\n",
              " 'batchnorm_enabled': True,\n",
              " 'bias': 0.0,\n",
              " 'dropout_keep_prob': 0.999,\n",
              " 'experiment_dir': 'test_experiment',\n",
              " 'l2_strength': 4e-05,\n",
              " 'learning_rate': 0.001,\n",
              " 'max_to_keep': 4,\n",
              " 'num_classes': 1001,\n",
              " 'num_epochs': 100,\n",
              " 'pretrained_path': 'pretrained_weights/mobilenet_v1.pkl',\n",
              " 'save_model_every': 5,\n",
              " 'shuffle': True,\n",
              " 'test_every': 5,\n",
              " 'to_test': True,\n",
              " 'to_train': False,\n",
              " 'width_multiplier': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main**"
      ],
      "metadata": {
        "id": "dJ3d0adC6bpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow version: 2.7.0\n",
        "\n",
        "def main():\n",
        "    # Parse the JSON arguments\n",
        "    try:\n",
        "        config_args = parse_args()\n",
        "    except:\n",
        "        print(\"Add a config file using \\'--config file_name.json\\'\")\n",
        "        exit(1)\n",
        "\n",
        "    # Create the experiment directories\n",
        "    _, config_args.summary_dir, config_args.checkpoint_dir = create_experiment_dirs(config_args.experiment_dir)\n",
        "\n",
        "    # Reset the default Tensorflow graph\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "\n",
        "    # Tensorflow specific configuration\n",
        "    config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
        "    config.gpu_options.allow_growth = True\n",
        "    sess = tf.compat.v1.Session(config=config)\n",
        "\n",
        "    # Data loading\n",
        "    data = DataLoader(config_args.batch_size, config_args.shuffle)\n",
        "    print(\"Loading Data...\")\n",
        "    config_args.img_height, config_args.img_width, config_args.num_channels, \\\n",
        "    config_args.train_data_size, config_args.test_data_size = data.load_data()\n",
        "    print(\"Data loaded\\n\\n\")\n",
        "\n",
        "    # Model creation\n",
        "    print(\"Building the model...\")\n",
        "    model = MobileNet(config_args)\n",
        "    print(\"Model is built successfully\\n\\n\")\n",
        "\n",
        "    # Summarizer creation\n",
        "    summarizer = Summarizer(sess, config_args.summary_dir)\n",
        "    # Train class\n",
        "    trainer = Train(sess, model, data, summarizer)\n",
        "\n",
        "    if config_args.to_train:\n",
        "        try:\n",
        "            print(\"Training...\")\n",
        "            trainer.train()\n",
        "            print(\"Training Finished\\n\\n\")\n",
        "        except KeyboardInterrupt:\n",
        "            trainer.save_model()\n",
        "\n",
        "    if config_args.to_test:\n",
        "        print(\"Final test!\")\n",
        "        trainer.test('val')\n",
        "        print(\"Testing Finished\\n\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "GaUPnhe_6eEv",
        "outputId": "d5e16a2f-73cb-4bb9-a28d-c1bd2463916e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: ipykernel_launcher.py: command not found\n",
            "Add a config file using '--config file_name.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--version] [--config CONFIG]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-72987f71-107a-4b95-8a96-ee545aa65027.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-eb2b2e6ff7c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-eb2b2e6ff7c1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Create the experiment directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_experiment_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Reset the default Tensorflow graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'config_args' referenced before assignment"
          ]
        }
      ]
    }
  ]
}